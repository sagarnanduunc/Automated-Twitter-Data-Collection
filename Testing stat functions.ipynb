{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from collections import Counter\n",
    "from json.decoder import JSONDecodeError\n",
    "import itertools as it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHashtagCounts(hastagList):\n",
    "    allHashtags = list(map(lambda x:x.lower(),hastagList))\n",
    "    return Counter(allHashtags)\n",
    "\n",
    "def getCoOccurrence(df,numField,field):\n",
    "    hashtagsCoOccurence = {}\n",
    "    for index, row in df.loc[df[numField]>1].iterrows(): # pick tweets with more than one hastag\n",
    "        for h1,h2 in list(it.combinations(row[field], 2)): # Find all combinations for the list of hashtag\n",
    "            #frozenset makes sure that order of the keys don't matter\n",
    "            if frozenset([h1,h2]) in hashtagsCoOccurence.keys() and len(frozenset([h1,h2]))>1: \n",
    "                hashtagsCoOccurence[frozenset([h1,h2])] +=1\n",
    "            else:\n",
    "                hashtagsCoOccurence[frozenset([h1,h2])] = 1\n",
    "    return Counter(hashtagsCoOccurence)\n",
    "\n",
    "def getTokenFreq(df, dataField, isList):\n",
    "    allTokens = []\n",
    "    if isList:\n",
    "        for x in df[dataField]:#.apply(lambda x: x.split(\",\")):\n",
    "            allTokens+=x\n",
    "    else:\n",
    "        for x in df[dataField].apply(lambda x: x.split(\",\")):\n",
    "            allTokens+=x\n",
    "    return getHashtagCounts(allTokens)\n",
    "\n",
    "def getDataOnKeyword(df,field,keyword): # Make sure that field you are searching in is a list\n",
    "    return df[df[field].apply(lambda x: True if keyword in x else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allFiles = []\n",
    "for file in os.listdir(\"./data\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        allFiles.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "tweets = {\"tweetId\":[],\"postedTime\":[],\"body\":[],\"hashtags\":[]}\n",
    "fileNo = 0\n",
    "for fileName in allFiles:\n",
    "    fileNo+=1\n",
    "    print(fileNo)\n",
    "    data = []\n",
    "    with open('./data/'+fileName) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except JSONDecodeError:\n",
    "                continue\n",
    "            \n",
    "            if \"lang\" not in data.keys():\n",
    "                continue\n",
    "            if data[\"lang\"] == \"en\" and len(data[\"entities\"][\"hashtags\"])>0:\n",
    "                tweets[\"tweetId\"].append(data[\"id_str\"])\n",
    "                tweets[\"postedTime\"].append(data[\"created_at\"])\n",
    "                tweets[\"body\"].append(data[\"text\"])\n",
    "                hashtags = []\n",
    "                for h in data[\"entities\"][\"hashtags\"]:\n",
    "                    hashtags.append(h[\"text\"])\n",
    "                tweets[\"hashtags\"].append(hashtags)\n",
    "            \n",
    "            \n",
    "            #jsonData = json.loads(line)\n",
    "            #data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'indices': [63, 76], 'text': 'MasterChefMx'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i=0\n",
    "i+=1\n",
    "while len(data[i][\"entities\"][\"hashtags\"]) == 0:\n",
    "    i+=1\n",
    "\n",
    "data[i][\"entities\"][\"hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open('./data/'+allFiles[46]) as f:\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "        except JSONDecodeError:\n",
    "            continue\n",
    "        #ptn\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets[\"numHashtags\"] = []\n",
    "for hashlist in tweets[\"hashtags\"]:\n",
    "    tweets[\"numHashtags\"].append(len(hashlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetDf = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetDf[\"hashtags\"] = tweetDf[\"hashtags\"].apply(lambda x: \",\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetDf.to_csv(\"./data/tweets25.csv\",quoting=csv.QUOTE_NONNUMERIC, date_format='%Y-%m-%d %H:%M:%S', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkData = pd.read_csv(\"./data/tweets25.csv\", encoding='utf-8',index_col=0)\n",
    "checkData[\"hashtags\"] = checkData[\"hashtags\"].apply(lambda x: str(x).lower())\n",
    "checkData[\"hashtags\"] = checkData[\"hashtags\"].apply(lambda x: x.split(\",\"))\n",
    "checkData[\"body\"] = checkData[\"body\"].apply(lambda x: x.lower())\n",
    "checkData[\"postedTime\"] = pd.to_datetime(checkData[\"postedTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allHashtags = []\n",
    "for x in checkData[\"hashtags\"]:#.apply(lambda x: x.split(\",\")):\n",
    "    allHashtags+=x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allHashtagsCtr = getTokenFreq(checkData,\"hashtags\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('btshot100', 3752),\n",
       " ('takeaknee', 3182),\n",
       " ('taketheknee', 2230),\n",
       " ('bts', 2090),\n",
       " ('nowplaying', 1890),\n",
       " ('teamladygaga', 1875),\n",
       " ('hot100', 1859),\n",
       " ('teenwolf', 1854),\n",
       " ('nfl', 1803),\n",
       " ('puertorico', 1699)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTemp.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allHashtagsCtr = getHashtagCounts(allHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('btshot100', 3752),\n",
       " ('takeaknee', 3182),\n",
       " ('taketheknee', 2230),\n",
       " ('bts', 2090),\n",
       " ('nowplaying', 1890),\n",
       " ('teamladygaga', 1875),\n",
       " ('hot100', 1859),\n",
       " ('teenwolf', 1854),\n",
       " ('nfl', 1803),\n",
       " ('puertorico', 1699),\n",
       " ('giveaway', 1698),\n",
       " ('mondaymotivation', 1659),\n",
       " ('grahamcassidy', 1537),\n",
       " ('starmagicballfanfave', 1322),\n",
       " ('job', 1319),\n",
       " ('방탄소년단', 1252),\n",
       " ('aldubgenuinefeelings', 1216),\n",
       " ('win', 1206),\n",
       " ('got7', 1200),\n",
       " ('exo', 1154),\n",
       " ('maga', 1055),\n",
       " ('boycottnfl', 1000),\n",
       " ('hiring', 1000),\n",
       " ('news', 976),\n",
       " ('trapadrive', 928),\n",
       " ('갓세븐', 911),\n",
       " ('raw', 894),\n",
       " ('np', 854),\n",
       " ('trecru', 825),\n",
       " ('7for7', 814),\n",
       " ('standforouranthem', 768),\n",
       " ('dalvsaz', 710),\n",
       " ('trump', 675),\n",
       " ('aldubloveyourown', 661),\n",
       " ('music', 632),\n",
       " ('bitcoin', 611),\n",
       " ('soundcloud', 609),\n",
       " ('lab17', 597),\n",
       " ('dna', 596),\n",
       " ('ai', 576),\n",
       " ('blockchain', 569),\n",
       " ('rt', 569),\n",
       " ('free', 561),\n",
       " ('jobs', 559),\n",
       " ('ad', 558),\n",
       " ('spyder', 551),\n",
       " ('youare', 541),\n",
       " ('marketing', 523),\n",
       " ('breaking', 514),\n",
       " ('dallascowboys', 514),\n",
       " ('quote', 508),\n",
       " ('travel', 507),\n",
       " ('yemeninquirynow', 504),\n",
       " ('lay', 503),\n",
       " ('iot', 485),\n",
       " ('tech', 474),\n",
       " ('business', 461),\n",
       " ('wwenomercy', 448),\n",
       " ('follow', 443),\n",
       " ('defrancoshowforstreamys', 440),\n",
       " ('healthcaredebate', 433),\n",
       " ('careerarc', 421),\n",
       " ('fashion', 419),\n",
       " ('nuest_w', 413),\n",
       " ('kaepernick', 406),\n",
       " ('nbamediaday', 404),\n",
       " ('mytipsformentalhealth', 404),\n",
       " ('mnf', 403),\n",
       " ('love', 396),\n",
       " ('bigolive', 395),\n",
       " ('ineedu', 389),\n",
       " ('billboardnews', 389),\n",
       " ('워너원', 387),\n",
       " ('teenwolffinale', 387),\n",
       " ('competition', 384),\n",
       " ('张艺兴', 381),\n",
       " ('wannaone', 379),\n",
       " ('mpn', 372),\n",
       " ('startrekdiscovery', 370),\n",
       " ('manimonday', 368),\n",
       " ('nationalcomicbookday', 368),\n",
       " ('usa', 363),\n",
       " ('sp7forstreamys', 361),\n",
       " ('tvxq', 355),\n",
       " ('ico', 353),\n",
       " ('photography', 353),\n",
       " ('retweet', 352),\n",
       " ('allrise', 349),\n",
       " ('art', 348),\n",
       " ('cheerupadn', 347),\n",
       " ('mgwv', 345),\n",
       " ('w_here', 322),\n",
       " ('iheartfestival', 318),\n",
       " ('donoharm', 316),\n",
       " ('mkjfambamblues', 313),\n",
       " ('socialmedia', 312),\n",
       " ('health', 309),\n",
       " ('需要你', 306),\n",
       " ('yemen', 302),\n",
       " ('nflboycott', 300)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allHashtagsCtr.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkData[\"hashtags\"] = checkData[\"hashtags\"].apply(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204413"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startData = checkData[checkData[\"hashtags\"].apply(lambda x: True if \"takeaknee\" in x else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startData = getDataOnKeyword(checkData,\"hashtags\",\"takeaknee\")\n",
    "allHashtagsCtr = getTokenFreq(startData,\"hashtags\",1)\n",
    "hashtagCoOccur = getCoOccurrence(startData,\"numHashtags\",\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3174"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(startData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('takeaknee', 3182),\n",
       " ('nfl', 143),\n",
       " ('taketheknee', 93),\n",
       " ('boycottnfl', 79),\n",
       " ('goodtrouble', 53),\n",
       " ('jerryjones', 41),\n",
       " ('mnf', 36),\n",
       " ('maga', 33),\n",
       " ('takeastand', 26),\n",
       " ('trump', 24)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allHashtagsCtr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'nfl', 'takeaknee'}), 143),\n",
       " (frozenset({'takeaknee', 'taketheknee'}), 93),\n",
       " (frozenset({'boycottnfl', 'takeaknee'}), 79),\n",
       " (frozenset({'goodtrouble', 'takeaknee'}), 53),\n",
       " (frozenset({'jerryjones', 'takeaknee'}), 41),\n",
       " (frozenset({'mnf', 'takeaknee'}), 36),\n",
       " (frozenset({'maga', 'takeaknee'}), 33),\n",
       " (frozenset({'takeaknee', 'takeastand'}), 26),\n",
       " (frozenset({'takeaknee', 'trump'}), 24),\n",
       " (frozenset({'dallascowboys', 'takeaknee'}), 22)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtagCoOccur.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtagList = []\n",
    "for x in startData[\"hashtags\"]:\n",
    "    hashtagList+=x\n",
    "hashtagListCtr = getHashtagCounts(hashtagList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('takeaknee', 3182),\n",
       " ('nfl', 143),\n",
       " ('taketheknee', 93),\n",
       " ('boycottnfl', 79),\n",
       " ('goodtrouble', 53),\n",
       " ('jerryjones', 41),\n",
       " ('mnf', 36),\n",
       " ('maga', 33),\n",
       " ('takeastand', 26),\n",
       " ('trump', 24),\n",
       " ('dallascowboys', 22),\n",
       " ('mondaynightfootball', 21),\n",
       " ('blacklivesmatter', 18),\n",
       " ('takeakneenfl', 18),\n",
       " ('standforouranthem', 17)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtagListCtr.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtagCoOccur = getCoOccurrence(startData,\"numHashtags\",\"hashtags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'nfl', 'takeaknee'}), 143),\n",
       " (frozenset({'takeaknee', 'taketheknee'}), 93),\n",
       " (frozenset({'boycottnfl', 'takeaknee'}), 79),\n",
       " (frozenset({'goodtrouble', 'takeaknee'}), 53),\n",
       " (frozenset({'jerryjones', 'takeaknee'}), 41),\n",
       " (frozenset({'mnf', 'takeaknee'}), 36),\n",
       " (frozenset({'maga', 'takeaknee'}), 33),\n",
       " (frozenset({'takeaknee', 'takeastand'}), 26),\n",
       " (frozenset({'takeaknee', 'trump'}), 24),\n",
       " (frozenset({'dallascowboys', 'takeaknee'}), 22)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtagCoOccur.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model, this can take some time...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f6696fc733b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'repl.py'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'repl.py'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'exec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Admin\\Desktop\\Study\\ML\\Project\\Automated-Twitter-Data-Collection\\repl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./word2vec_twitter_model.bin\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading the model, this can take some time...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The vocabulary size is: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Admin\\Desktop\\Study\\ML\\Project\\Automated-Twitter-Data-Collection\\word2vecReader.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, norm_only)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mbinary_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer1_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exec(compile(open('repl.py', \"rb\").read(), 'repl.py', 'exec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./glove.twitter.27B/w2v.twitter.27B.200d.txt', binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.7419614791870117),\n",
       " (';', 0.6766710877418518),\n",
       " ('<repeat>', 0.6555314064025879),\n",
       " ('?', 0.6401088237762451),\n",
       " ('ok', 0.6091393828392029),\n",
       " ('\"', 0.6055634021759033),\n",
       " ('<user>', 0.5923637747764587),\n",
       " ('rt', 0.5861671566963196),\n",
       " (':', 0.5844361782073975),\n",
       " ('&', 0.5822509527206421)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([\",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "tknz = TweetTokenizer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = [\"rt\"] #+ stopwords.words('english') + list(string.punctuation)\n",
    "from gensim.models.doc2vec import (\n",
    "    Doc2Vec,\n",
    "    TaggedDocument,\n",
    ")\n",
    "# translator = str.maketrans('', '', string.punctuation.replace(\"#\",\"\").replace(\"@\",\"\").replace(\"'\",\"\"))\n",
    "translator = str.maketrans('', '', \"@#\")\n",
    "\n",
    "\n",
    "def cleanTweet(text):\n",
    "    text = (re.sub(r\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\W/]*))*\", \"\", text)).lower() #removes urls\n",
    "    #text = re.sub(r'[^\\x00-\\x7F]+',' ', text) # removes unicodes (emogis)\n",
    "    text = re.sub(r'(\\r)|(\\n)','',text) # removes newline characters\n",
    "    text = text.replace(\"rt \",\"\")\n",
    "    text= text.translate(translator) \n",
    "#     tokens = word_tokenize(text)\n",
    "    tokens = tknz.tokenize(text)\n",
    "#     temp=[]\n",
    "#     for i in tokens:\n",
    "#         if not i in stop:\n",
    "#             temp.append(i)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# def labelizeDocs(dataList, label):\n",
    "#     import gensim\n",
    "#     LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "#     labelized = []\n",
    "#     for dataPoint in dataList:\n",
    "#         labelized.append(LabeledSentence(dataPoint, [label]))\n",
    "    \n",
    "#     return labelized\n",
    "\n",
    "\n",
    "def dataPrep(df,dataField):\n",
    "    dataList = list(df[dataField].apply(lambda x: cleanTweet(x)))\n",
    "    return dataList\n",
    "\n",
    "def TaggDocs(dataList,label):\n",
    "    newDataList = []\n",
    "    for dataPoint in dataList:\n",
    "        newDataList.append(TaggedDocument(dataPoint, tags=[label]))\n",
    "    return newDataList\n",
    "\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.concatenate(vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'hashtags', 'numHashtags', 'postedTime', 'tweetId'], dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4979"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabDict = dict(getHashtagCounts(np.sum(currentDataList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startData = getDataOnKeyword(checkData,\"hashtags\",\"takeaknee\")\n",
    "currentDataList = dataPrep(startData,\"body\")\n",
    "currentData = TaggDocs(currentDataList,\"takeaknee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelD2V = Doc2Vec(vector_size = 150,min_count=1, workers=1)\n",
    "\n",
    "modelD2V.build_vocab(currentData)\n",
    "modelD2V.train(currentData, total_examples=modelD2V.corpus_count , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"-- Base model\")\n",
    "print(\"Vocabulary:\", str(modelD2V.vocabulary))\n",
    "print(\"Tags:\", str(modelD2V.docvecs.doctags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cum_table': array([  43009431,   83489087,  116522242, ..., 2147285948, 2147384797,\n",
       "        2147483647], dtype=uint32),\n",
       " 'max_vocab_size': None,\n",
       " 'min_count': 1,\n",
       " 'null_word': 0,\n",
       " 'raw_vocab': defaultdict(int, {}),\n",
       " 'sample': 0.001,\n",
       " 'sorted_vocab': True}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelD2V.vocabulary.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startData = getDataOnKeyword(checkData,\"hashtags\",\"nfl\")\n",
    "currentDataList = dataPrep(startData,\"body\")\n",
    "currentData = TaggDocs(currentDataList,\"nfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelD2V.build_vocab(currentData, update=True)\n",
    "modelD2V.train(currentData, total_examples=modelD2V.corpus_count , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "similarity() missing 1 required positional argument: 'w2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cbe958a5b976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelD2V\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'takeaknee'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: similarity() missing 1 required positional argument: 'w2'"
     ]
    }
   ],
   "source": [
    "modelD2V.wv.similarity('takeaknee',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14946002,  0.12858914, -0.11064957,  0.18100592,  0.06236762,\n",
       "       -0.18181707, -0.0905913 ,  0.10728567, -0.05289251,  0.16354413,\n",
       "       -0.05712029,  0.0676381 , -0.20695189, -0.15659273, -0.13450722,\n",
       "       -0.15902491,  0.13131812, -0.10548742,  0.15689614, -0.11913933,\n",
       "        0.08753449, -0.08534686, -0.0040849 ,  0.15699062,  0.06726166,\n",
       "       -0.16104774,  0.20500545, -0.10828539, -0.06238169, -0.12093531,\n",
       "       -0.14034757, -0.17053373,  0.04061782, -0.11544313, -0.15224779,\n",
       "       -0.23189385, -0.2721566 ,  0.16871324,  0.25340274,  0.12745458,\n",
       "       -0.03096354, -0.09017185,  0.14774773, -0.17303638, -0.12251407,\n",
       "       -0.01375806, -0.06732075, -0.09686725,  0.1435098 ,  0.2123388 ,\n",
       "       -0.09106575, -0.05397035,  0.09362867, -0.14578848,  0.3861563 ,\n",
       "        0.0199358 ,  0.10730179, -0.06666312, -0.06678216, -0.09857281,\n",
       "       -0.03787049,  0.21551861,  0.35128236,  0.08892806, -0.00980853,\n",
       "        0.02956393, -0.0170132 , -0.01389343,  0.0733055 ,  0.16835979,\n",
       "       -0.1108927 ,  0.09110709, -0.00236013, -0.12252197, -0.07787796,\n",
       "       -0.04241395, -0.20100574,  0.08776799,  0.09086742, -0.08426327,\n",
       "        0.1455788 , -0.1898002 , -0.14679064, -0.10495716, -0.09287248,\n",
       "        0.1562248 , -0.14235854,  0.09463864,  0.02565496,  0.01008012,\n",
       "       -0.1196679 ,  0.01927488,  0.19408007, -0.05599014, -0.02533038,\n",
       "        0.00811303,  0.18640634, -0.05122101, -0.24687491,  0.0668353 ,\n",
       "       -0.08720262, -0.07491328,  0.20105493,  0.19686335, -0.06063877,\n",
       "       -0.12412294, -0.00248717, -0.27627513, -0.02960155, -0.02620596,\n",
       "       -0.13704908, -0.09804459,  0.07513802, -0.23658878,  0.0784838 ,\n",
       "        0.0888949 , -0.29073435,  0.06218703,  0.08384822, -0.17389265,\n",
       "        0.03393667, -0.11702772, -0.1759458 ,  0.2697533 ,  0.02357029,\n",
       "        0.13611211, -0.19266921, -0.18306679,  0.19368005,  0.29013488,\n",
       "        0.01140547,  0.01496092, -0.17395116,  0.25737315, -0.04138412,\n",
       "        0.12771869,  0.02655433, -0.17139633, -0.14506103, -0.09456733,\n",
       "       -0.2570498 ,  0.02921905,  0.11837804, -0.06283452,  0.04865604,\n",
       "        0.1328263 , -0.19713396, -0.06792281,  0.29305404, -0.15519837],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelD2V.infer_vector(currentData[1].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec1 = modelD2V.infer_vector(currentData[0].words)\n",
    "vec2 = modelD2V.infer_vector(currentData[1].words)\n",
    "#modelD2V.wv.wmdistance(currentData[0].words,currentData[1].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#modelD2V.docvecs.similar_by_vector(modelD2V.infer_vector(currentData[1].words),modelD2V.infer_vector(currentData[1].words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['1', ')', 'as', 'i', 'am', 'being', 'told', 'to', 'keep', 'quiet', ',', 'it', 'occurs', 'to', 'me', 'that', 'blessed', 'are', 'the', 'meek', 'who', \"don't\", 'stir', 'the', 'pot', '.', 'takeaknee'], tags=['takeaknee'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentData[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ranks = []\n",
    "# second_ranks = []\n",
    "# for doc_id in range(len(currentData)):\n",
    "#     inferred_vector = modelD2V.infer_vector(currentData[doc_id].words)\n",
    "#     sims = modelD2V.docvecs.most_similar([inferred_vector], topn=len(modelD2V.docvecs))\n",
    "#     print(sims)\n",
    "# #     rank = [docid for docid, sim in sims].index(doc_id)\n",
    "# #     ranks.append(rank)\n",
    "    \n",
    "# #     second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "modelW2V= Word2Vec()\n",
    "modelW2V.build_vocab_from_freq(vocabDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brennanmgilmore',\n",
       " ':',\n",
       " 'my',\n",
       " 'grandpa',\n",
       " 'is',\n",
       " 'a',\n",
       " '97',\n",
       " 'year-old',\n",
       " 'wwii',\n",
       " 'vet',\n",
       " '&',\n",
       " 'missouri',\n",
       " 'farmer',\n",
       " 'who',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'join',\n",
       " 'w',\n",
       " '/',\n",
       " 'those',\n",
       " 'who',\n",
       " 'takeaknee',\n",
       " ':',\n",
       " '\"',\n",
       " 'those',\n",
       " 'kids',\n",
       " 'have',\n",
       " '…']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentDataList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.05464102e-03,  2.27787020e-03, -2.32855906e-03, -9.30631883e-04,\n",
       "       -2.75504310e-03, -6.96565490e-04,  2.41199392e-03,  1.51247112e-03,\n",
       "        2.08194717e-03, -4.72134503e-04,  5.53112011e-04,  3.03332042e-03,\n",
       "        3.21036601e-03,  6.18466293e-04,  5.23625233e-04,  2.32679420e-03,\n",
       "       -2.17999122e-03, -3.16101545e-03, -1.63674331e-03, -3.06758890e-03,\n",
       "        2.17901776e-03, -1.53846422e-03,  1.72459695e-03, -1.44430948e-03,\n",
       "       -1.90755993e-03, -1.44501356e-03,  1.97073817e-03,  7.22338227e-05,\n",
       "        1.50917750e-03,  8.89170740e-04,  7.97304208e-04, -4.75994195e-04,\n",
       "        2.65102275e-03,  1.24810508e-03,  2.87641352e-03, -1.07126754e-04,\n",
       "        2.03426834e-03, -1.19043025e-03, -1.19020150e-03, -1.69440324e-03,\n",
       "       -2.89054005e-03, -2.79267016e-03, -2.74410169e-03, -1.15914889e-04,\n",
       "        2.18764506e-03,  1.53955992e-03,  2.52419710e-03,  2.47155013e-03,\n",
       "        3.31399613e-03,  2.46036984e-03, -1.91608886e-03, -9.61819082e-04,\n",
       "       -5.16408822e-04,  1.96111132e-03, -2.46862858e-03, -9.00435960e-04,\n",
       "       -3.01511958e-03, -3.17250378e-03,  1.30464823e-03,  4.92033025e-04,\n",
       "        1.85540377e-03, -3.00188689e-03,  6.23776286e-04, -3.02964053e-03,\n",
       "       -6.99168522e-05, -1.42676826e-03, -6.46694447e-04, -2.33267690e-03,\n",
       "       -9.32803203e-04,  1.91948039e-03,  2.11573974e-03,  1.53404730e-03,\n",
       "       -2.56919675e-03, -3.30333761e-03,  1.10527850e-03, -2.07187631e-03,\n",
       "       -3.27712414e-03, -2.75652017e-03,  3.04350106e-04, -2.40454319e-04,\n",
       "        1.45823648e-03, -1.63369172e-03, -1.45938000e-04,  4.48689389e-05,\n",
       "       -8.20637972e-04,  2.86275847e-03,  1.40671164e-03, -5.55531471e-04,\n",
       "        5.48950229e-05, -2.03800388e-03,  2.89743789e-03,  1.78758695e-03,\n",
       "        1.88537105e-03, -2.28118082e-03,  2.31593358e-03, -7.81058334e-05,\n",
       "       -3.01342644e-03, -1.68242655e-03,  2.77207862e-03, -6.24199805e-04,\n",
       "       -1.13391096e-03, -4.12388734e-04, -7.90053920e-04, -4.54035559e-04,\n",
       "       -9.14659526e-04,  3.05338902e-03, -2.73230858e-03, -1.72288925e-03,\n",
       "       -2.18489929e-03, -9.02164495e-04,  2.68267724e-03, -1.74084550e-03,\n",
       "        1.18080189e-03,  2.27028783e-03, -7.19274743e-04, -7.66825979e-04,\n",
       "        1.07400876e-03,  2.58756359e-03,  4.14094189e-04,  2.33366256e-04,\n",
       "        7.36122427e-04, -1.45910098e-03,  2.61236704e-03, -1.49229052e-03,\n",
       "        3.11813061e-03, -9.76702664e-04,  2.30506528e-03, -2.28273356e-03,\n",
       "        1.88611785e-03,  3.06195905e-03,  2.70175608e-03,  1.41795666e-04,\n",
       "        5.94528334e-04,  2.12987326e-03, -3.29430564e-03,  2.15920061e-03,\n",
       "       -2.25407814e-04,  3.49144742e-04, -2.36212183e-03,  8.47436313e-04,\n",
       "       -1.70791405e-03, -2.32431199e-03, -1.33576768e-03, -1.29122706e-03,\n",
       "        3.24858516e-03,  1.41404092e-03, -1.76187186e-03,  2.77485093e-03,\n",
       "       -8.95073055e-04, -2.07633898e-03], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelD2V.infer_vector(['abcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dm = gensim.models.Doc2Vec(size=size, min_count=1, iter=100, alpha=0.025, min_alpha=0.0001)\n",
    "model_dbow = gensim.models.Doc2Vec(size=size, min_count=1, dm=0, iter=100,alpha=0.025, min_alpha=0.0001)\n",
    "    \n",
    "\n",
    "model_dm.build_vocab(allData)\n",
    "model_dbow.build_vocab(allData)   \n",
    "\n",
    "    \n",
    "model_dm.train(allData, total_examples=model_dm.corpus_count , epochs=100)   ######## allData not only training\n",
    "model_dbow.train(allData, total_examples=model_dm.corpus_count , epochs=100)##### instead of epochs=model_dm.iter\n",
    "\n",
    "\n",
    "\n",
    "train_vecs_dm = np.array(getVecs(model_dm, training, size))\n",
    "train_vecs_dbow = np.array(getVecs(model_dbow, training, size))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-02 23:33:35,641 : MainThread : INFO : collecting all words and their counts\n",
      "2018-05-02 23:33:35,643 : MainThread : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-05-02 23:33:35,644 : MainThread : INFO : collected 46 word types and 2 unique tags from a corpus of 3 examples and 57 words\n",
      "2018-05-02 23:33:35,644 : MainThread : INFO : Loading a fresh vocabulary\n",
      "2018-05-02 23:33:35,645 : MainThread : INFO : min_count=1 retains 46 unique words (100% of original 46, drops 0)\n",
      "2018-05-02 23:33:35,646 : MainThread : INFO : min_count=1 leaves 57 word corpus (100% of original 57, drops 0)\n",
      "2018-05-02 23:33:35,647 : MainThread : INFO : deleting the raw counts dictionary of 46 items\n",
      "2018-05-02 23:33:35,647 : MainThread : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2018-05-02 23:33:35,648 : MainThread : INFO : downsampling leaves estimated 14 word corpus (25.7% of prior 57)\n",
      "2018-05-02 23:33:35,649 : MainThread : INFO : estimated required memory for 46 words and 100 dimensions: 61000 bytes\n",
      "2018-05-02 23:33:35,650 : MainThread : INFO : resetting layer weights\n",
      "2018-05-02 23:33:35,651 : MainThread : INFO : training model with 1 workers on 46 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-02 23:33:35,653 : Thread-7 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,654 : Thread-6 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,655 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,656 : MainThread : INFO : EPOCH - 1 : training on 57 raw words (14 effective words) took 0.0s, 9432 effective words/s\n",
      "2018-05-02 23:33:35,658 : Thread-9 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,659 : Thread-8 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,660 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,662 : MainThread : INFO : EPOCH - 2 : training on 57 raw words (18 effective words) took 0.0s, 4974 effective words/s\n",
      "2018-05-02 23:33:35,664 : Thread-11 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,665 : Thread-10 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,665 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,667 : MainThread : INFO : EPOCH - 3 : training on 57 raw words (16 effective words) took 0.0s, 5528 effective words/s\n",
      "2018-05-02 23:33:35,668 : Thread-13 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,669 : Thread-12 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,669 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,671 : MainThread : INFO : EPOCH - 4 : training on 57 raw words (11 effective words) took 0.0s, 4167 effective words/s\n",
      "2018-05-02 23:33:35,672 : Thread-15 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,673 : Thread-14 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,673 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,674 : MainThread : INFO : EPOCH - 5 : training on 57 raw words (16 effective words) took 0.0s, 15924 effective words/s\n",
      "2018-05-02 23:33:35,676 : Thread-17 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,676 : Thread-16 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,676 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,678 : MainThread : INFO : EPOCH - 6 : training on 57 raw words (19 effective words) took 0.0s, 8362 effective words/s\n",
      "2018-05-02 23:33:35,679 : Thread-19 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,680 : Thread-18 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,680 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,682 : MainThread : INFO : EPOCH - 7 : training on 57 raw words (18 effective words) took 0.0s, 6415 effective words/s\n",
      "2018-05-02 23:33:35,686 : Thread-21 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,688 : Thread-20 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,688 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,693 : MainThread : INFO : EPOCH - 8 : training on 57 raw words (23 effective words) took 0.0s, 3627 effective words/s\n",
      "2018-05-02 23:33:35,695 : Thread-23 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,697 : Thread-22 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,697 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,699 : MainThread : INFO : EPOCH - 9 : training on 57 raw words (22 effective words) took 0.0s, 6211 effective words/s\n",
      "2018-05-02 23:33:35,702 : Thread-25 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,704 : Thread-24 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,704 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,706 : MainThread : INFO : EPOCH - 10 : training on 57 raw words (18 effective words) took 0.0s, 4721 effective words/s\n",
      "2018-05-02 23:33:35,709 : Thread-27 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,713 : Thread-26 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,714 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,717 : MainThread : INFO : EPOCH - 11 : training on 57 raw words (23 effective words) took 0.0s, 3142 effective words/s\n",
      "2018-05-02 23:33:35,719 : Thread-29 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,721 : Thread-28 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,721 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,722 : MainThread : INFO : EPOCH - 12 : training on 57 raw words (17 effective words) took 0.0s, 6443 effective words/s\n",
      "2018-05-02 23:33:35,724 : Thread-31 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,724 : Thread-30 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,724 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,726 : MainThread : INFO : EPOCH - 13 : training on 57 raw words (14 effective words) took 0.0s, 5415 effective words/s\n",
      "2018-05-02 23:33:35,728 : Thread-33 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,729 : Thread-32 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,729 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,730 : MainThread : INFO : EPOCH - 14 : training on 57 raw words (22 effective words) took 0.0s, 9530 effective words/s\n",
      "2018-05-02 23:33:35,732 : Thread-35 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,733 : Thread-34 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,733 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,734 : MainThread : INFO : EPOCH - 15 : training on 57 raw words (18 effective words) took 0.0s, 9117 effective words/s\n",
      "2018-05-02 23:33:35,735 : Thread-37 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,736 : Thread-36 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,736 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,737 : MainThread : INFO : EPOCH - 16 : training on 57 raw words (19 effective words) took 0.0s, 9297 effective words/s\n",
      "2018-05-02 23:33:35,738 : Thread-39 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,739 : Thread-38 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,739 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,740 : MainThread : INFO : EPOCH - 17 : training on 57 raw words (22 effective words) took 0.0s, 10874 effective words/s\n",
      "2018-05-02 23:33:35,741 : Thread-41 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,742 : Thread-40 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,743 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,744 : MainThread : INFO : EPOCH - 18 : training on 57 raw words (23 effective words) took 0.0s, 11592 effective words/s\n",
      "2018-05-02 23:33:35,745 : Thread-43 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,745 : Thread-42 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,745 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,747 : MainThread : INFO : EPOCH - 19 : training on 57 raw words (14 effective words) took 0.0s, 7601 effective words/s\n",
      "2018-05-02 23:33:35,748 : Thread-45 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,749 : Thread-44 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,749 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,750 : MainThread : INFO : EPOCH - 20 : training on 57 raw words (21 effective words) took 0.0s, 9692 effective words/s\n",
      "2018-05-02 23:33:35,751 : Thread-47 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,752 : Thread-46 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,752 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,754 : MainThread : INFO : EPOCH - 21 : training on 57 raw words (23 effective words) took 0.0s, 9982 effective words/s\n",
      "2018-05-02 23:33:35,755 : Thread-49 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,756 : Thread-48 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,756 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,757 : MainThread : INFO : EPOCH - 22 : training on 57 raw words (18 effective words) took 0.0s, 8797 effective words/s\n",
      "2018-05-02 23:33:35,759 : Thread-51 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,759 : Thread-50 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,760 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,761 : MainThread : INFO : EPOCH - 23 : training on 57 raw words (17 effective words) took 0.0s, 8143 effective words/s\n",
      "2018-05-02 23:33:35,762 : Thread-53 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,763 : Thread-52 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,764 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,764 : MainThread : INFO : EPOCH - 24 : training on 57 raw words (19 effective words) took 0.0s, 9045 effective words/s\n",
      "2018-05-02 23:33:35,766 : Thread-55 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,766 : Thread-54 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,767 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,768 : MainThread : INFO : EPOCH - 25 : training on 57 raw words (13 effective words) took 0.0s, 5777 effective words/s\n",
      "2018-05-02 23:33:35,769 : Thread-57 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,770 : Thread-56 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,770 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,771 : MainThread : INFO : EPOCH - 26 : training on 57 raw words (24 effective words) took 0.0s, 14229 effective words/s\n",
      "2018-05-02 23:33:35,772 : Thread-59 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,773 : Thread-58 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,773 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,775 : MainThread : INFO : EPOCH - 27 : training on 57 raw words (18 effective words) took 0.0s, 7932 effective words/s\n",
      "2018-05-02 23:33:35,776 : Thread-61 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,778 : Thread-60 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,778 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,779 : MainThread : INFO : EPOCH - 28 : training on 57 raw words (18 effective words) took 0.0s, 7039 effective words/s\n",
      "2018-05-02 23:33:35,780 : Thread-63 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,781 : Thread-62 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,781 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,782 : MainThread : INFO : EPOCH - 29 : training on 57 raw words (15 effective words) took 0.0s, 7452 effective words/s\n",
      "2018-05-02 23:33:35,784 : Thread-65 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,785 : Thread-64 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,785 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,786 : MainThread : INFO : EPOCH - 30 : training on 57 raw words (17 effective words) took 0.0s, 9770 effective words/s\n",
      "2018-05-02 23:33:35,787 : Thread-67 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,787 : Thread-66 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,788 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,789 : MainThread : INFO : EPOCH - 31 : training on 57 raw words (18 effective words) took 0.0s, 9156 effective words/s\n",
      "2018-05-02 23:33:35,790 : Thread-69 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,791 : Thread-68 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,791 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,792 : MainThread : INFO : EPOCH - 32 : training on 57 raw words (17 effective words) took 0.0s, 7529 effective words/s\n",
      "2018-05-02 23:33:35,794 : Thread-71 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,794 : Thread-70 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,794 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,796 : MainThread : INFO : EPOCH - 33 : training on 57 raw words (17 effective words) took 0.0s, 7013 effective words/s\n",
      "2018-05-02 23:33:35,797 : Thread-73 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,798 : Thread-72 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,798 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,799 : MainThread : INFO : EPOCH - 34 : training on 57 raw words (20 effective words) took 0.0s, 10220 effective words/s\n",
      "2018-05-02 23:33:35,800 : Thread-75 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,801 : Thread-74 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,801 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,802 : MainThread : INFO : EPOCH - 35 : training on 57 raw words (19 effective words) took 0.0s, 11091 effective words/s\n",
      "2018-05-02 23:33:35,803 : Thread-77 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,804 : Thread-76 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,804 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,805 : MainThread : INFO : EPOCH - 36 : training on 57 raw words (19 effective words) took 0.0s, 11145 effective words/s\n",
      "2018-05-02 23:33:35,806 : Thread-79 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,807 : Thread-78 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,807 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,808 : MainThread : INFO : EPOCH - 37 : training on 57 raw words (19 effective words) took 0.0s, 9891 effective words/s\n",
      "2018-05-02 23:33:35,809 : Thread-81 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,810 : Thread-80 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,810 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,811 : MainThread : INFO : EPOCH - 38 : training on 57 raw words (19 effective words) took 0.0s, 9806 effective words/s\n",
      "2018-05-02 23:33:35,812 : Thread-83 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,813 : Thread-82 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,813 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,814 : MainThread : INFO : EPOCH - 39 : training on 57 raw words (10 effective words) took 0.0s, 5320 effective words/s\n",
      "2018-05-02 23:33:35,816 : Thread-85 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,817 : Thread-84 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,817 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,818 : MainThread : INFO : EPOCH - 40 : training on 57 raw words (21 effective words) took 0.0s, 8974 effective words/s\n",
      "2018-05-02 23:33:35,820 : Thread-87 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,820 : Thread-86 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,821 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,822 : MainThread : INFO : EPOCH - 41 : training on 57 raw words (21 effective words) took 0.0s, 9080 effective words/s\n",
      "2018-05-02 23:33:35,823 : Thread-89 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,824 : Thread-88 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,825 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,826 : MainThread : INFO : EPOCH - 42 : training on 57 raw words (16 effective words) took 0.0s, 6648 effective words/s\n",
      "2018-05-02 23:33:35,827 : Thread-91 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,828 : Thread-90 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,828 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,830 : MainThread : INFO : EPOCH - 43 : training on 57 raw words (20 effective words) took 0.0s, 8553 effective words/s\n",
      "2018-05-02 23:33:35,831 : Thread-93 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,832 : Thread-92 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,832 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,833 : MainThread : INFO : EPOCH - 44 : training on 57 raw words (8 effective words) took 0.0s, 4367 effective words/s\n",
      "2018-05-02 23:33:35,834 : Thread-95 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,835 : Thread-94 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,835 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,837 : MainThread : INFO : EPOCH - 45 : training on 57 raw words (14 effective words) took 0.0s, 6010 effective words/s\n",
      "2018-05-02 23:33:35,838 : Thread-97 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,839 : Thread-96 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,840 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,841 : MainThread : INFO : EPOCH - 46 : training on 57 raw words (15 effective words) took 0.0s, 6241 effective words/s\n",
      "2018-05-02 23:33:35,842 : Thread-99 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,843 : Thread-98 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,843 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,845 : MainThread : INFO : EPOCH - 47 : training on 57 raw words (19 effective words) took 0.0s, 7174 effective words/s\n",
      "2018-05-02 23:33:35,846 : Thread-101 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,847 : Thread-100 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,847 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,849 : MainThread : INFO : EPOCH - 48 : training on 57 raw words (21 effective words) took 0.0s, 10236 effective words/s\n",
      "2018-05-02 23:33:35,850 : Thread-103 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,851 : Thread-102 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,851 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,852 : MainThread : INFO : EPOCH - 49 : training on 57 raw words (17 effective words) took 0.0s, 9622 effective words/s\n",
      "2018-05-02 23:33:35,854 : Thread-105 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,855 : Thread-104 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,855 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,856 : MainThread : INFO : EPOCH - 50 : training on 57 raw words (18 effective words) took 0.0s, 8015 effective words/s\n",
      "2018-05-02 23:33:35,857 : Thread-107 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,858 : Thread-106 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,858 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,859 : MainThread : INFO : EPOCH - 51 : training on 57 raw words (16 effective words) took 0.0s, 7053 effective words/s\n",
      "2018-05-02 23:33:35,861 : Thread-109 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,861 : Thread-108 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,861 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,863 : MainThread : INFO : EPOCH - 52 : training on 57 raw words (16 effective words) took 0.0s, 9894 effective words/s\n",
      "2018-05-02 23:33:35,864 : Thread-111 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,865 : Thread-110 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,865 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,866 : MainThread : INFO : EPOCH - 53 : training on 57 raw words (21 effective words) took 0.0s, 8866 effective words/s\n",
      "2018-05-02 23:33:35,867 : Thread-113 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,868 : Thread-112 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,869 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,870 : MainThread : INFO : EPOCH - 54 : training on 57 raw words (19 effective words) took 0.0s, 8064 effective words/s\n",
      "2018-05-02 23:33:35,871 : Thread-115 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,872 : Thread-114 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,873 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,874 : MainThread : INFO : EPOCH - 55 : training on 57 raw words (14 effective words) took 0.0s, 9382 effective words/s\n",
      "2018-05-02 23:33:35,876 : Thread-117 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,877 : Thread-116 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,877 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,878 : MainThread : INFO : EPOCH - 56 : training on 57 raw words (22 effective words) took 0.0s, 11420 effective words/s\n",
      "2018-05-02 23:33:35,879 : Thread-119 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,880 : Thread-118 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,880 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,881 : MainThread : INFO : EPOCH - 57 : training on 57 raw words (15 effective words) took 0.0s, 6438 effective words/s\n",
      "2018-05-02 23:33:35,882 : Thread-121 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,884 : Thread-120 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,884 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,885 : MainThread : INFO : EPOCH - 58 : training on 57 raw words (14 effective words) took 0.0s, 6423 effective words/s\n",
      "2018-05-02 23:33:35,886 : Thread-123 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,887 : Thread-122 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,887 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,889 : MainThread : INFO : EPOCH - 59 : training on 57 raw words (15 effective words) took 0.0s, 7079 effective words/s\n",
      "2018-05-02 23:33:35,890 : Thread-125 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,892 : Thread-124 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,892 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,894 : MainThread : INFO : EPOCH - 60 : training on 57 raw words (16 effective words) took 0.0s, 5050 effective words/s\n",
      "2018-05-02 23:33:35,895 : Thread-127 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,896 : Thread-126 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,896 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,898 : MainThread : INFO : EPOCH - 61 : training on 57 raw words (15 effective words) took 0.0s, 6698 effective words/s\n",
      "2018-05-02 23:33:35,899 : Thread-129 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,900 : Thread-128 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,901 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,902 : MainThread : INFO : EPOCH - 62 : training on 57 raw words (13 effective words) took 0.0s, 5043 effective words/s\n",
      "2018-05-02 23:33:35,903 : Thread-131 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,904 : Thread-130 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,904 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,907 : MainThread : INFO : EPOCH - 63 : training on 57 raw words (23 effective words) took 0.0s, 5801 effective words/s\n",
      "2018-05-02 23:33:35,910 : Thread-133 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,911 : Thread-132 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,911 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,914 : MainThread : INFO : EPOCH - 64 : training on 57 raw words (20 effective words) took 0.0s, 5805 effective words/s\n",
      "2018-05-02 23:33:35,914 : Thread-135 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,914 : Thread-134 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,915 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,917 : MainThread : INFO : EPOCH - 65 : training on 57 raw words (20 effective words) took 0.0s, 8642 effective words/s\n",
      "2018-05-02 23:33:35,918 : Thread-137 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,919 : Thread-136 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,920 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,921 : MainThread : INFO : EPOCH - 66 : training on 57 raw words (18 effective words) took 0.0s, 7479 effective words/s\n",
      "2018-05-02 23:33:35,922 : Thread-139 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,923 : Thread-138 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,923 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,924 : MainThread : INFO : EPOCH - 67 : training on 57 raw words (22 effective words) took 0.0s, 14463 effective words/s\n",
      "2018-05-02 23:33:35,926 : Thread-141 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,927 : Thread-140 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,927 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,928 : MainThread : INFO : EPOCH - 68 : training on 57 raw words (20 effective words) took 0.0s, 8571 effective words/s\n",
      "2018-05-02 23:33:35,930 : Thread-143 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,930 : Thread-142 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,931 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,932 : MainThread : INFO : EPOCH - 69 : training on 57 raw words (19 effective words) took 0.0s, 8162 effective words/s\n",
      "2018-05-02 23:33:35,933 : Thread-145 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,934 : Thread-144 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,934 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,935 : MainThread : INFO : EPOCH - 70 : training on 57 raw words (18 effective words) took 0.0s, 9511 effective words/s\n",
      "2018-05-02 23:33:35,937 : Thread-147 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,937 : Thread-146 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,937 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,938 : MainThread : INFO : EPOCH - 71 : training on 57 raw words (16 effective words) took 0.0s, 8827 effective words/s\n",
      "2018-05-02 23:33:35,940 : Thread-149 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,940 : Thread-148 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,940 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,942 : MainThread : INFO : EPOCH - 72 : training on 57 raw words (15 effective words) took 0.0s, 7580 effective words/s\n",
      "2018-05-02 23:33:35,943 : Thread-151 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,944 : Thread-150 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,944 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,945 : MainThread : INFO : EPOCH - 73 : training on 57 raw words (19 effective words) took 0.0s, 9899 effective words/s\n",
      "2018-05-02 23:33:35,946 : Thread-153 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,947 : Thread-152 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,947 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,948 : MainThread : INFO : EPOCH - 74 : training on 57 raw words (12 effective words) took 0.0s, 6059 effective words/s\n",
      "2018-05-02 23:33:35,949 : Thread-155 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,950 : Thread-154 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,950 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,951 : MainThread : INFO : EPOCH - 75 : training on 57 raw words (16 effective words) took 0.0s, 8135 effective words/s\n",
      "2018-05-02 23:33:35,953 : Thread-157 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,954 : Thread-156 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,954 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,954 : MainThread : INFO : EPOCH - 76 : training on 57 raw words (19 effective words) took 0.0s, 11205 effective words/s\n",
      "2018-05-02 23:33:35,955 : Thread-159 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,956 : Thread-158 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,956 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,957 : MainThread : INFO : EPOCH - 77 : training on 57 raw words (18 effective words) took 0.0s, 9549 effective words/s\n",
      "2018-05-02 23:33:35,959 : Thread-161 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,960 : Thread-160 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,960 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,961 : MainThread : INFO : EPOCH - 78 : training on 57 raw words (23 effective words) took 0.0s, 12171 effective words/s\n",
      "2018-05-02 23:33:35,962 : Thread-163 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,963 : Thread-162 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,963 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,964 : MainThread : INFO : EPOCH - 79 : training on 57 raw words (13 effective words) took 0.0s, 5812 effective words/s\n",
      "2018-05-02 23:33:35,966 : Thread-165 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,967 : Thread-164 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,967 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,968 : MainThread : INFO : EPOCH - 80 : training on 57 raw words (16 effective words) took 0.0s, 7457 effective words/s\n",
      "2018-05-02 23:33:35,971 : Thread-167 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,972 : Thread-166 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,972 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,974 : MainThread : INFO : EPOCH - 81 : training on 57 raw words (22 effective words) took 0.0s, 7458 effective words/s\n",
      "2018-05-02 23:33:35,976 : Thread-169 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,977 : Thread-168 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,977 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,979 : MainThread : INFO : EPOCH - 82 : training on 57 raw words (18 effective words) took 0.0s, 6276 effective words/s\n",
      "2018-05-02 23:33:35,980 : Thread-171 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,981 : Thread-170 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,981 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,983 : MainThread : INFO : EPOCH - 83 : training on 57 raw words (14 effective words) took 0.0s, 5581 effective words/s\n",
      "2018-05-02 23:33:35,984 : Thread-173 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,986 : Thread-172 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,986 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,988 : MainThread : INFO : EPOCH - 84 : training on 57 raw words (16 effective words) took 0.0s, 5473 effective words/s\n",
      "2018-05-02 23:33:35,989 : Thread-175 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,990 : Thread-174 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,990 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,992 : MainThread : INFO : EPOCH - 85 : training on 57 raw words (14 effective words) took 0.0s, 5311 effective words/s\n",
      "2018-05-02 23:33:35,993 : Thread-177 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,994 : Thread-176 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,994 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,996 : MainThread : INFO : EPOCH - 86 : training on 57 raw words (19 effective words) took 0.0s, 8443 effective words/s\n",
      "2018-05-02 23:33:35,997 : Thread-179 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:35,997 : Thread-178 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:35,997 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:35,998 : MainThread : INFO : EPOCH - 87 : training on 57 raw words (18 effective words) took 0.0s, 11210 effective words/s\n",
      "2018-05-02 23:33:35,999 : Thread-181 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,000 : Thread-180 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,000 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,001 : MainThread : INFO : EPOCH - 88 : training on 57 raw words (23 effective words) took 0.0s, 15657 effective words/s\n",
      "2018-05-02 23:33:36,002 : Thread-183 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,003 : Thread-182 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,003 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,004 : MainThread : INFO : EPOCH - 89 : training on 57 raw words (20 effective words) took 0.0s, 10957 effective words/s\n",
      "2018-05-02 23:33:36,005 : Thread-185 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,005 : Thread-184 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,006 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,006 : MainThread : INFO : EPOCH - 90 : training on 57 raw words (15 effective words) took 0.0s, 9358 effective words/s\n",
      "2018-05-02 23:33:36,008 : Thread-187 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,008 : Thread-186 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,009 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,010 : MainThread : INFO : EPOCH - 91 : training on 57 raw words (14 effective words) took 0.0s, 5119 effective words/s\n",
      "2018-05-02 23:33:36,012 : Thread-189 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,014 : Thread-188 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,014 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,015 : MainThread : INFO : EPOCH - 92 : training on 57 raw words (21 effective words) took 0.0s, 7418 effective words/s\n",
      "2018-05-02 23:33:36,017 : Thread-191 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,018 : Thread-190 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,018 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,019 : MainThread : INFO : EPOCH - 93 : training on 57 raw words (15 effective words) took 0.0s, 6798 effective words/s\n",
      "2018-05-02 23:33:36,020 : Thread-193 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,021 : Thread-192 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,021 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,023 : MainThread : INFO : EPOCH - 94 : training on 57 raw words (20 effective words) took 0.0s, 8627 effective words/s\n",
      "2018-05-02 23:33:36,024 : Thread-195 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,024 : Thread-194 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,024 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,026 : MainThread : INFO : EPOCH - 95 : training on 57 raw words (17 effective words) took 0.0s, 7982 effective words/s\n",
      "2018-05-02 23:33:36,027 : Thread-197 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,028 : Thread-196 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,029 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,030 : MainThread : INFO : EPOCH - 96 : training on 57 raw words (14 effective words) took 0.0s, 4561 effective words/s\n",
      "2018-05-02 23:33:36,032 : Thread-199 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,033 : Thread-198 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,033 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,034 : MainThread : INFO : EPOCH - 97 : training on 57 raw words (17 effective words) took 0.0s, 7921 effective words/s\n",
      "2018-05-02 23:33:36,035 : Thread-201 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,036 : Thread-200 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,037 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,038 : MainThread : INFO : EPOCH - 98 : training on 57 raw words (19 effective words) took 0.0s, 7395 effective words/s\n",
      "2018-05-02 23:33:36,040 : Thread-203 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,041 : Thread-202 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,041 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,042 : MainThread : INFO : EPOCH - 99 : training on 57 raw words (19 effective words) took 0.0s, 8703 effective words/s\n",
      "2018-05-02 23:33:36,043 : Thread-205 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,044 : Thread-204 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,045 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,046 : MainThread : INFO : EPOCH - 100 : training on 57 raw words (12 effective words) took 0.0s, 5172 effective words/s\n",
      "2018-05-02 23:33:36,046 : MainThread : INFO : training on a 5700 raw words (1768 effective words) took 0.4s, 4480 effective words/s\n",
      "2018-05-02 23:33:36,047 : MainThread : INFO : collecting all words and their counts\n",
      "2018-05-02 23:33:36,047 : MainThread : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 2 tags\n",
      "2018-05-02 23:33:36,048 : MainThread : INFO : collected 28 word types and 3 unique tags from a corpus of 2 examples and 29 words\n",
      "2018-05-02 23:33:36,048 : MainThread : INFO : Updating model with new vocabulary\n",
      "2018-05-02 23:33:36,048 : MainThread : INFO : New added 28 unique words (50% of original 56) and increased the count of 28 pre-existing words (50% of original 56)\n",
      "2018-05-02 23:33:36,049 : MainThread : INFO : deleting the raw counts dictionary of 28 items\n",
      "2018-05-02 23:33:36,049 : MainThread : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2018-05-02 23:33:36,050 : MainThread : INFO : downsampling leaves estimated 11 word corpus (39.0% of prior 29)\n",
      "2018-05-02 23:33:36,050 : MainThread : INFO : estimated required memory for 56 words and 100 dimensions: 74600 bytes\n",
      "2018-05-02 23:33:36,051 : MainThread : INFO : updating layer weights\n",
      "2018-05-02 23:33:36,052 : MainThread : INFO : training model with 1 workers on 68 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-02 23:33:36,053 : Thread-207 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,054 : Thread-206 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,054 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,056 : MainThread : INFO : EPOCH - 1 : training on 29 raw words (5 effective words) took 0.0s, 2355 effective words/s\n",
      "2018-05-02 23:33:36,057 : Thread-209 : DEBUG : job loop exiting, total 1 jobs\n",
      "2018-05-02 23:33:36,058 : Thread-208 : DEBUG : worker exiting, processed 1 jobs\n",
      "2018-05-02 23:33:36,058 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 23:33:36,059 : MainThread : INFO : EPOCH - 2 : training on 29 raw words (11 effective words) took 0.0s, 4769 effective words/s\n",
      "2018-05-02 23:33:36,060 : Thread-211 : DEBUG : job loop exiting, total 1 jobs\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from gensim.models.doc2vec import (\n",
    "    Doc2Vec,\n",
    "    TaggedDocument,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(threadName)s : %(levelname)s : %(message)s',\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "\n",
    "def to_str(d):\n",
    "    return \", \".join(d.keys())\n",
    "\n",
    "\n",
    "SENTS = [\n",
    "    \"anecdotal using a personal experience or an isolated example instead of a sound argument or compelling evidence\",\n",
    "    \"plausible thinking that just because something is plausible means that it is true\",\n",
    "    \"occam razor is used as a heuristic technique discovery tool to guide scientists in the development of theoretical models rather than as an arbiter between published models\",\n",
    "    \"karl popper argues that a preference for simple theories need not appeal to practical or aesthetic considerations\",\n",
    "    \"the successful prediction of a stock future price could yield significant profit\",\n",
    "]\n",
    "\n",
    "SENTS = [s.split() for s in SENTS]\n",
    "\n",
    "\n",
    "\n",
    "sentences_1 = [\n",
    "        TaggedDocument(SENTS[0], tags=['SENT_0']),\n",
    "        TaggedDocument(SENTS[1], tags=['SENT_0']),\n",
    "        TaggedDocument(SENTS[2], tags=['SENT_1']),\n",
    "    ]\n",
    "\n",
    "sentences_2 = [\n",
    "        TaggedDocument(SENTS[3], tags=['SENT_1']),\n",
    "        TaggedDocument(SENTS[4], tags=['SENT_2']),\n",
    "    ]\n",
    "\n",
    "model = Doc2Vec(min_count=1, workers=1)\n",
    "\n",
    "model.build_vocab(sentences_1)\n",
    "model.train(sentences_1,total_examples=model.corpus_count,epochs=100)\n",
    "\n",
    "print(\"-- Base model\")\n",
    "# print(\"Vocabulary:\", to_str(model.vocab))\n",
    "print(\"Tags:\", to_str(model.docvecs.doctags))\n",
    "\n",
    "model.build_vocab(sentences_2, update=True)\n",
    "model.train(sentences_2,total_examples=model.corpus_count,epochs=100)\n",
    "\n",
    "print(\"-- Updated model\")\n",
    "# print(\"Vocabulary:\", to_str(model.vocab))\n",
    "print(\"Tags:\", to_str(model.docvecs.doctags))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
